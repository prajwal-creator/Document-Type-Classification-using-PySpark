{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ec3189",
   "metadata": {},
   "source": [
    "# Document Type Classification using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e6697",
   "metadata": {},
   "source": [
    "#### The Project is based on Distributed Big Data Technology.\n",
    "\n",
    "The objective of this project is to create a Machine Learning(ML) model by performing data intensive computing using Pyspark which involves handling data acquired from NewYorkTimes API.\n",
    "\n",
    "#### Document Classification :\n",
    "Document Classification analogous to general classification of instances, deals with assigning labels to documents. The documents can be of different length, structure(can be written by different authors using variety of writing styles) and source.\n",
    "Here multi-class classification is used to classify the document types into the respective categories they belong to. Different types of models/classifiers are defined to predict the categories of the articles and their accuracy are studied.\n",
    "\n",
    "One of the main goals of this project is to optimize the handling and processing of huge data and to observe it using Spark environment .\n",
    "\n",
    "#### Apache Spark:\n",
    "Apache Spark is a Big Data framework which operates on distributed data collections. It furnishes in-memory computations for improved and quicker data processing over MapReduce. It is a cluster-computing framework which is designed for faster data computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a887",
   "metadata": {},
   "source": [
    "## Setting Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121d4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:/extras/spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeaf736",
   "metadata": {},
   "source": [
    "## Creating Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a59a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    "                          .master(\"local\")\\\n",
    "                          .appName('DOC_classifier')\\\n",
    "                          .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1fcdc",
   "metadata": {},
   "source": [
    "## Importing necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa74e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020884aa",
   "metadata": {},
   "source": [
    "## Setting SQL Context "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a97590",
   "metadata": {},
   "source": [
    "A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716abc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44d501",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838e266",
   "metadata": {},
   "source": [
    "#### Data consists of text files based on following class:\n",
    "#### 1) Fashion 2) Technology 3) Science 4) Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfd259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fas_df = spark.read.text('Data/Fashion/*')\n",
    "fas_df = fas_df.withColumn(\"category\",lit(\"Fashion\"))\n",
    "\n",
    "tech_df = spark.read.text('Data/Technology/*')\n",
    "tech_df = tech_df.withColumn(\"category\",lit(\"Technology\"))\n",
    "\n",
    "sci_df = spark.read.text('Data/Science/*')\n",
    "sci_df = sci_df.withColumn(\"category\",lit(\"science\"))\n",
    "\n",
    "mov_df = spark.read.text('Data/Movie/*')\n",
    "mov_df = mov_df.withColumn(\"category\",lit(\"Movie\"))\n",
    "\n",
    "\n",
    "merge_df1 = fas_df.union(tech_df)\n",
    "merge_df2 = merge_df1.union(sci_df)\n",
    "merge_df3 = merge_df2.union(mov_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46898d",
   "metadata": {},
   "source": [
    "#### Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb295cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               value|category|\n",
      "+--------------------+--------+\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = merge_df3.select([column for column in merge_df3.columns])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bace8cf",
   "metadata": {},
   "source": [
    "#### Final Unknown Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51853548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               value|category|\n",
      "+--------------------+--------+\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Fas_udf = spark.read.text('Data/unknown/Fashion/*')\n",
    "Fas_udf = Fas_udf.withColumn(\"category\",lit(\"Fashion\"))\n",
    "\n",
    "science_udf = spark.read.text('Data/unknown/science/*')\n",
    "science_udf = science_udf.withColumn(\"category\",lit(\"science\"))\n",
    "\n",
    "tech_udf = spark.read.text('Data/unknown/technology/*')\n",
    "tech_udf = tech_udf.withColumn(\"category\",lit(\"technology\"))\n",
    "\n",
    "movie_udf = spark.read.text('Data/unknown/Movie/*')\n",
    "movie_udf = movie_udf.withColumn(\"category\",lit(\"Movie\"))\n",
    "\n",
    "merge_udf1 = Fas_udf.union(science_udf)\n",
    "merge_udf2 = merge_udf1.union(tech_udf)\n",
    "merge_udf3 = merge_udf2.union(movie_udf)\n",
    "\n",
    "unknown_data = merge_udf3.select([column for column in merge_udf3.columns])\n",
    "unknown_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8b561",
   "metadata": {},
   "source": [
    "## Setting Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71d9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = {'metric':['Train_Accuracy', 'Test_Accuracy','Unknown_Accuracy']}\n",
    "metric_df=pd.DataFrame(data_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe14b24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train_Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknown_Accuracy</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [Train_Accuracy, Test_Accuracy, Unknown_Accuracy]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.set_index(['metric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f5d25",
   "metadata": {},
   "source": [
    "## Data Manipulation using Pyspark libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e60b7",
   "metadata": {},
   "source": [
    "### RegexTokenizer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7600d",
   "metadata": {},
   "source": [
    "A regex based tokenizer that extracts tokens either by using the provided regex pattern (in Java dialect) to split the text (default) or repeatedly matching the regex (if gaps is false). Optional parameters also allow filtering tokens using a minimal length. It returns an array of strings that can be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e116b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"value\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c10547",
   "metadata": {},
   "source": [
    "### Downloading Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2754bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prajw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608f87b",
   "metadata": {},
   "source": [
    "### StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ddd72",
   "metadata": {},
   "source": [
    "A feature transformer that filters out stop words from input. Since 3.0.0, StopWordsRemover can filter out multiple columns at once by setting the inputCols parameter. Note that when both the inputCol and inputCols parameters are set, an Exception will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e250ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopwords=nltk.corpus.stopwords.words('english')\n",
    "add_stopwords_1 = [\"nytimes\",\"com\",\"sense\",\"day\",\"common\",\"business\",\"todays\",\"said\",\"food\",\"review\",\"sunday\",\"letters\",\"politics\",\"events\",\"terms\",\"services\",\"years\",\"contributors\",\"companies\",\"listings\",\"applications\",\"tax\",\"trump\",\"president\",\"contributing\",\"make\",\"think\",\"woman\",\"federal\",\"called\",\"system\",\"found\",\"american\",\"sale\",\"headline\",\"arts\",\"times\",\"subscriptions\",\"choices\",\"privacy\",\"take\",\"jobs\",\"books\",\"account\",\"accounts\",\"television\",\"nyc\",\"writers\",\"multimedia\",\"journeys\",\"editorials\",\"photography\",\"automobiles\",\"paper\",\"city\",\"tool\",\"sports\",\"weddings\",\"columnists\",\"contribution\",\"even\",\"nyt\",\"obituary\",\"state\",\"travel\",\"advertise\",\"pm\",\"street\",\"go\",\"corrections\",\"saturday\",\"company\",\"dance\",\"states\",\"real\",\"movies\",\"estate\",\"percent\",\"music\",\"tech\",\"living\",\"science\",\"fashion\",\"please\",\"opinion\",\"art\",\"new\",\"york\",\"time\",\"u\",\"wa\",\"reading\",\"ha\",\"video\",\"image\",\"photo\",\"credit\",\"edition\",\"magazine\",\"oped\",\"could\",\"crossword\",\"mr\",\"term\",\"feedback\",\"index\",\"get\",\"also\",\"b\",\"help\",\"year\",\"health\",\"united\",\"education\",\"week\",\"think\",\"guide\",\"event\",\"two\",\"first\",\"subscription\",\"service\",\"cut\",\"is\",\"nytimescom\",\"section\",\"sections\",\"Sections\",\"Home\",\"home\",\"Search\",\"search\",\"Skip\",\"skip\",\"content\",\"navigation\",\"View\",\"view\",\"mobile\",\"version\",\"Subscribe\",\"subscribe\",\"Now\",\"now\",\"Log\",\"log\",\"In\",\"in\",\"setting\",\"settings\",\"Site\",\"site\",\"Loading\",\"loading\",\"article\",\"next\",\"previous\",\"Advertisement\",\"ad\",\"advertisement\",\"Supported\",\"supported\",\"by\",\"Share\",\"share\",\"Page\",\"page\",\"Continue\",\"continue\",\"main\",\"story\",\"newsletter\",\"Sign\",\"Up\",\"Manage\",\"email\",\"preferences\",\"Not\",\"you\",\"opt\",\"out\",\"contact\",\"us\",\"anytime\",\"thank\",\"subscribing\",\"see\",\"more\",\"email\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered1\").setStopWords(add_stopwords)\n",
    "stopwordsRemover1 = StopWordsRemover(inputCol=\"filtered1\", outputCol=\"filtered\").setStopWords(add_stopwords_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99cd918",
   "metadata": {},
   "source": [
    "# StringIndexer, HashingTF and IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834ebcf",
   "metadata": {},
   "source": [
    "StringIndexer :A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values.\n",
    "\n",
    "Hashingtf:Maps a sequence of terms to their term frequencies using the hashing trick. Currently we use Austin Appleby’s MurmurHash 3 algorithm (MurmurHash3_x86_32) to calculate the hash code value for the term object. Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the numFeatures parameter; otherwise the features will not be mapped evenly to the columns.\n",
    "\n",
    "IDF: he standard formulation is used: idf = log((m + 1) / (d(t) + 1)), where m is the total number of documents and d(t) is the number of documents that contain term t.This implementation supports filtering out terms which do not appear in a minimum number of documents (controlled by the variable minDocFreq). For terms that are not in at least minDocFreq documents, the IDF is found as 0, resulting in TF-IDFs of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80359599",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288cfb0",
   "metadata": {},
   "source": [
    "Pipeline :A simple pipeline, which acts as an estimator. A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer. When Pipeline.fit() is called, the stages are executed in order. If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model. Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages. If stages is an empty list, the pipeline acts as an identity transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b48e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover,stopwordsRemover1, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80e74b",
   "metadata": {},
   "source": [
    "## Data Transformation and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d210477",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ea373",
   "metadata": {},
   "source": [
    "## Logistic Rgression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfdef22",
   "metadata": {},
   "source": [
    "Logistic regression is a popular method to predict a categorical response. It is a special case of Generalized Linear models that predicts the probability of the outcomes. In spark.ml logistic regression can be used to predict a binary outcome by using binomial logistic regression, or it can be used to predict a multiclass outcome by using multinomial logistic regression. Use the family parameter to select between these two algorithms, or leave it unset and Spark will infer the correct variant.\n",
    "\n",
    "Multinomial logistic regression can be used for binary classification by setting the family param to “multinomial”. It will produce two sets of coefficients and two intercepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b58fc9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9529e6",
   "metadata": {},
   "source": [
    "### Performance on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e183820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9822394802109057,0.00235...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9811567547470802,0.00302...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9770537920390813,0.00631...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9763162809849949,0.01115...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9762569248827576,0.00562...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.970554468408183,0.009335...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9679333738863134,0.00870...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9669405158721934,0.00738...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9635934697853594,0.00953...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.957076942799982,0.008945...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_train = lrModel.transform(trainingData)\n",
    "predictions_train.filter(predictions_train['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94578472",
   "metadata": {},
   "source": [
    "### Train Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0680cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Accuracy of train data using logistic_regression-----: 98.76222962433778%\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "train_accuracy= evaluator.evaluate(predictions_train)*100\n",
    "print(\"-------Accuracy of train data using logistic_regression-----: \" + str(evaluator.evaluate(predictions_train)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5da7a3",
   "metadata": {},
   "source": [
    "### Performance on Test Data and Unknown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8debf96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9754632623041882,0.01057...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9431747402417925,0.02006...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9172397458630736,0.02471...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9053576601908154,0.02005...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.8962780254409616,0.02195...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.8507710146790355,0.01021...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.8454293725021643,0.05217...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.8197670154180591,0.01462...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.8190208403033724,0.04383...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.8129886620616786,0.04727...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e98a5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2490fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+-----+----------+\n",
      "|value|category|probability|label|prediction|\n",
      "+-----+--------+-----------+-----+----------+\n",
      "+-----+--------+-----------+-----+----------+\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|               value|category|               words|           filtered1|            filtered|         rawFeatures|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, industry,...|(1000,[10,12,43,5...|(1000,[10,12,43,5...|  3.0|[0.01492090532267...|[0.21386014488012...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, governors...|(1000,[0,19,78,80...|(1000,[0,19,78,80...|  3.0|[-0.2878215105794...|[0.15284112935317...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, luncheon,...|(1000,[12,19,48,7...|(1000,[12,19,48,7...|  3.0|[-0.0988087482643...|[0.13868385320448...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions2 = lrModel.transform(unknown_dataset)\n",
    "predictions2.filter(predictions2['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "predictions2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079999d4",
   "metadata": {},
   "source": [
    "### Test and Unknown Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Accuracy of test data using logistic_regression-----: 64.6386141204794%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using logistic_regression-----: \" + str(evaluator.evaluate(predictions)*100)+\"%\")\n",
    "test_accuracy= evaluator.evaluate(predictions)*100\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using logistic_regression-----: \" + str(evaluator.evaluate(predictions2)*100)+\"%\")\n",
    "u_accuracy= evaluator.evaluate(predictions2)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19200e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df['Logistic Regression'] = [train_accuracy,test_accuracy,u_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b0f09c",
   "metadata": {},
   "source": [
    "##  Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa53a2",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of simple probabilistic, multiclass classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between every pair of features.\n",
    "\n",
    "Naive Bayes can be trained very efficiently. With a single pass over the training data, it computes the conditional probability distribution of each feature given each label. For prediction, it applies Bayes’ theorem to compute the conditional probability distribution of each label given an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bee2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552b684",
   "metadata": {},
   "source": [
    "### Performace on Train Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dfa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = model.transform(trainingData)\n",
    "predictions_train.filter(predictions_train['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f2be3",
   "metadata": {},
   "source": [
    "### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "train_accuracy= evaluator.evaluate(predictions_train)*100\n",
    "print(\"-------Accuracy of train data using Naive Bayes Classifier-----: \" + str(evaluator.evaluate(predictions_train)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef18a1b",
   "metadata": {},
   "source": [
    "### Performance on Test Data and Unknown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ddbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = model.transform(testData)\n",
    "predictions3.filter(predictions3['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "predictions3.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4 = model.transform(unknown_dataset)\n",
    "predictions4.filter(predictions4['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "predictions4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106f4f1",
   "metadata": {},
   "source": [
    "### Test and Unknown Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using naive_bayes-----: \" + str(evaluator.evaluate(predictions3)*100)+\"%\")\n",
    "test_accuracy= evaluator.evaluate(predictions3)*100\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using naive_bayes-----: \" + str(evaluator.evaluate(predictions4)*100)+\"%\")\n",
    "u_accuracy= evaluator.evaluate(predictions4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df['Naive Bayes'] = [train_accuracy,test_accuracy,u_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9dddf",
   "metadata": {},
   "source": [
    "##  Decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a86c45",
   "metadata": {},
   "source": [
    "Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "pipelineFit_dt = pipeline.fit(data)\n",
    "dataset = pipelineFit_dt.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "dt = DecisionTreeClassifier(impurity=\"gini\")\n",
    "dtModel = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf9b61",
   "metadata": {},
   "source": [
    "### Performance on Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547229e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = dtModel.transform(trainingData)\n",
    "predictions_dt.filter(predictions_dt['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b585ae9",
   "metadata": {},
   "source": [
    "### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79311819",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of train data using Decision Tree-----: \" + str(evaluator.evaluate(predictions_dt)*100)+\"%\")\n",
    "train_accuracy= evaluator.evaluate(predictions_dt)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccd1f4",
   "metadata": {},
   "source": [
    "### Performace on Test and Unknown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)\n",
    "predictions2 = dtModel.transform(unknown_dataset)\n",
    "predictions2.filter(predictions2['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea79d5",
   "metadata": {},
   "source": [
    "### Test and Unknown Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using Decision Tree-----: \" + str(evaluator.evaluate(predictions)*100)+\"%\")\n",
    "test_accuracy= evaluator.evaluate(predictions)*100\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using Decision Tree-----: \" + str(evaluator.evaluate(predictions2)*100)+\"%\")\n",
    "u_accuracy= evaluator.evaluate(predictions2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df['Decision Tree'] = [train_accuracy,test_accuracy,u_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b27e7",
   "metadata": {},
   "source": [
    "## Random forest classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd86784",
   "metadata": {},
   "source": [
    "Random forests are ensembles of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. The spark.ml implementation supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "pipelineFit_rf = pipeline.fit(data)\n",
    "dataset = pipelineFit_rf.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "rf = RandomForestClassifier(numTrees=50)\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc02b5",
   "metadata": {},
   "source": [
    "### Performance on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_rf = rfModel.transform(trainingData)\n",
    "predictions_rf.filter(predictions_rf['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8127ef0",
   "metadata": {},
   "source": [
    "### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of train data using Random Forest-----: \" + str(evaluator.evaluate(predictions_rf)*100)+\"%\")\n",
    "train_accuracy= evaluator.evaluate(predictions_rf)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdd127",
   "metadata": {},
   "source": [
    "### Performance on Test and Unknown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ecc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d434789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)\n",
    "predictions2 = rfModel.transform(unknown_dataset)\n",
    "predictions2.filter(predictions2['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b814f4a",
   "metadata": {},
   "source": [
    "### Test and Unknown Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using Random Forest-----: \" + str(evaluator.evaluate(predictions)*100)+\"%\")\n",
    "test_accuracy= evaluator.evaluate(predictions)*100\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using Random Forest-----: \" + str(evaluator.evaluate(predictions2)*100)+\"%\")\n",
    "u_accuracy= evaluator.evaluate(predictions2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc452162",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df['Random Forest'] = [train_accuracy,test_accuracy,u_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25adea2",
   "metadata": {},
   "source": [
    "##  One-vs-Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4386e",
   "metadata": {},
   "source": [
    "OneVsRest is an example of a machine learning reduction for performing multiclass classification given a base classifier that can perform binary classification efficiently. It is also known as “One-vs-All\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b344843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "pipelineFit_svc = pipeline.fit(data)\n",
    "dataset = pipelineFit_svc.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "ovrModel = ovr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a30502",
   "metadata": {},
   "source": [
    "### Performance on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svc = ovrModel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a5ed7",
   "metadata": {},
   "source": [
    "### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "train_accuracy= evaluator.evaluate(predictions_svc)*100\n",
    "print(\"-------Accuracy of train data using One-vs-Rest-----: \" + str(train_accuracy)+\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91d728",
   "metadata": {},
   "source": [
    "### Performance on Test and Unknown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ovrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de67487",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)\n",
    "predictions2 = ovrModel.transform(unknown_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a51c16",
   "metadata": {},
   "source": [
    "### Test and Unknown Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae12283",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "test_accuracy= evaluator.evaluate(predictions)*100\n",
    "print(\"-------Accuracy of test data using One-vs-Rest-----: \" + str(test_accuracy)+\"%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "u_accuracy= evaluator.evaluate(predictions2)*100\n",
    "print(\"-------Accuracy of unknown data using One-vs-Rest-----: \" + str(u_accuracy)+\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df['One-vs-Rest'] = [train_accuracy,test_accuracy,u_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84136eeb",
   "metadata": {},
   "source": [
    "## Metric Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c365148",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
