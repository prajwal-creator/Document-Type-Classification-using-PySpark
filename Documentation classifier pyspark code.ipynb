{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121d4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:/extras/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a59a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    "                          .appName('classifier')\\\n",
    "                          .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa74e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716abc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfd259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fas_df = spark.read.text('Data/Fashion/*')\n",
    "fas_df = fas_df.withColumn(\"category\",lit(\"Fashion\"))\n",
    "\n",
    "tech_df = spark.read.text('Data/Technology/*')\n",
    "tech_df = tech_df.withColumn(\"category\",lit(\"Technology\"))\n",
    "\n",
    "sci_df = spark.read.text('Data/Science/*')\n",
    "sci_df = sci_df.withColumn(\"category\",lit(\"science\"))\n",
    "\n",
    "mov_df = spark.read.text('Data/Movie/*')\n",
    "mov_df = mov_df.withColumn(\"category\",lit(\"Movie\"))\n",
    "\n",
    "\n",
    "merge_df1 = fas_df.union(tech_df)\n",
    "merge_df2 = merge_df1.union(sci_df)\n",
    "merge_df3 = merge_df2.union(mov_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb295cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               value|category|\n",
      "+--------------------+--------+\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = merge_df3.select([column for column in merge_df3.columns])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51853548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               value|category|\n",
      "+--------------------+--------+\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "|   Sections SEARC...| Fashion|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Fas_udf = spark.read.text('Data/unknown/Fashion/*')\n",
    "Fas_udf = Fas_udf.withColumn(\"category\",lit(\"Fashion\"))\n",
    "\n",
    "science_udf = spark.read.text('Data/unknown/science/*')\n",
    "science_udf = science_udf.withColumn(\"category\",lit(\"science\"))\n",
    "\n",
    "tech_udf = spark.read.text('Data/unknown/technology/*')\n",
    "tech_udf = tech_udf.withColumn(\"category\",lit(\"technology\"))\n",
    "\n",
    "movie_udf = spark.read.text('Data/unknown/Movie/*')\n",
    "movie_udf = movie_udf.withColumn(\"category\",lit(\"Movie\"))\n",
    "\n",
    "merge_udf1 = Fas_udf.union(science_udf)\n",
    "merge_udf2 = merge_udf1.union(tech_udf)\n",
    "merge_udf3 = merge_udf2.union(movie_udf)\n",
    "\n",
    "unknown_data = merge_udf3.select([column for column in merge_udf3.columns])\n",
    "unknown_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e116b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"value\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e250ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prajw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "add_stopwords=nltk.corpus.stopwords.words('english')\n",
    "add_stopwords_1 = [\"nytimes\",\"com\",\"sense\",\"day\",\"common\",\"business\",\"todays\",\"said\",\"food\",\"review\",\"sunday\",\"letters\",\"politics\",\"events\",\"terms\",\"services\",\"years\",\"contributors\",\"companies\",\"listings\",\"applications\",\"tax\",\"trump\",\"president\",\"contributing\",\"make\",\"think\",\"woman\",\"federal\",\"called\",\"system\",\"found\",\"american\",\"sale\",\"headline\",\"arts\",\"times\",\"subscriptions\",\"choices\",\"privacy\",\"take\",\"jobs\",\"books\",\"account\",\"accounts\",\"television\",\"nyc\",\"writers\",\"multimedia\",\"journeys\",\"editorials\",\"photography\",\"automobiles\",\"paper\",\"city\",\"tool\",\"sports\",\"weddings\",\"columnists\",\"contribution\",\"even\",\"nyt\",\"obituary\",\"state\",\"travel\",\"advertise\",\"pm\",\"street\",\"go\",\"corrections\",\"saturday\",\"company\",\"dance\",\"states\",\"real\",\"movies\",\"estate\",\"percent\",\"music\",\"tech\",\"living\",\"science\",\"fashion\",\"please\",\"opinion\",\"art\",\"new\",\"york\",\"time\",\"u\",\"wa\",\"reading\",\"ha\",\"video\",\"image\",\"photo\",\"credit\",\"edition\",\"magazine\",\"oped\",\"could\",\"crossword\",\"mr\",\"term\",\"feedback\",\"index\",\"get\",\"also\",\"b\",\"help\",\"year\",\"health\",\"united\",\"education\",\"week\",\"think\",\"guide\",\"event\",\"two\",\"first\",\"subscription\",\"service\",\"cut\",\"is\",\"nytimescom\",\"section\",\"sections\",\"Sections\",\"Home\",\"home\",\"Search\",\"search\",\"Skip\",\"skip\",\"content\",\"navigation\",\"View\",\"view\",\"mobile\",\"version\",\"Subscribe\",\"subscribe\",\"Now\",\"now\",\"Log\",\"log\",\"In\",\"in\",\"setting\",\"settings\",\"Site\",\"site\",\"Loading\",\"loading\",\"article\",\"next\",\"previous\",\"Advertisement\",\"ad\",\"advertisement\",\"Supported\",\"supported\",\"by\",\"Share\",\"share\",\"Page\",\"page\",\"Continue\",\"continue\",\"main\",\"story\",\"newsletter\",\"Sign\",\"Up\",\"Manage\",\"email\",\"preferences\",\"Not\",\"you\",\"opt\",\"out\",\"contact\",\"us\",\"anytime\",\"thank\",\"subscribing\",\"see\",\"more\",\"email\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered1\").setStopWords(add_stopwords)\n",
    "stopwordsRemover1 = StopWordsRemover(inputCol=\"filtered1\", outputCol=\"filtered\").setStopWords(add_stopwords_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80359599",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover,stopwordsRemover1, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d210477",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1916e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9830929888325927,0.00384...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9747092140405847,0.00447...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9686060739262444,0.01916...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9681088802503716,0.00823...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.964292088183739,0.013876...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9606086171105256,0.00778...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9564970809495726,0.00653...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9523739507319724,0.01119...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.94913640953388,0.0234766...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9488224053408241,0.01420...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_train = lrModel.transform(trainingData)\n",
    "predictions_train.filter(predictions_train['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0be47594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Accuracy of train data using logistic_regression-----: 99.45378632158226%\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of train data using logistic_regression-----: \" + str(evaluator.evaluate(predictions_train)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8debf96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                         value|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9908482339340072,0.00182...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9802917286793247,0.00202...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|Technology|[0.9639835577417218,0.00295...|  2.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9497979743473339,0.00516...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9436546734364557,0.01587...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9357835995319901,0.04857...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9096375772642806,0.01534...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.8789536502476271,0.06299...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.8766610279476359,0.01445...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.8605474286777948,0.05332...|  0.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e98a5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2490fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+-----+----------+\n",
      "|value|category|probability|label|prediction|\n",
      "+-----+--------+-----------+-----+----------+\n",
      "+-----+--------+-----------+-----+----------+\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|               value|category|               words|           filtered1|            filtered|         rawFeatures|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, industry,...|(1000,[10,12,43,5...|(1000,[10,12,43,5...|  3.0|[-0.0950705146411...|[0.18761885123337...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, governors...|(1000,[0,19,78,80...|(1000,[0,19,78,80...|  3.0|[-0.3825284051792...|[0.13554434712800...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, luncheon,...|(1000,[12,19,48,7...|(1000,[12,19,48,7...|  3.0|[-0.2741982942206...|[0.11163673859795...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions2 = lrModel.transform(unknown_dataset)\n",
    "predictions2.filter(predictions2['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "predictions2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2e023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Accuracy of test data using logistic_regression-----: 62.19060109615789%\n",
      "-------Accuracy of unknown data using logistic_regression-----: 16.618357487922705%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using logistic_regression-----: \" + str(evaluator.evaluate(predictions)*100)+\"%\")\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using logistic_regression-----: \" + str(evaluator.evaluate(predictions2)*100)+\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9729a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the data -- Naive Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a718aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,9.95898746962288E-17,4...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,8.666383808393523E-17,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,5.83249218608599E-17,1...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,1.4102774227108478E-17...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,6.840979668391668E-19,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,4.410386468458676E-19,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,1.7012505618103597E-19...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,5.723041467306191E-20,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,1.744560461670642E-20,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,2.0037659727671827E-27...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of train data using logistic_regression-----: 94.85482995786275%\n"
     ]
    }
   ],
   "source": [
    "predictions_train = model.transform(trainingData)\n",
    "predictions_train.filter(predictions_train['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of train data using logistic_regression-----: \" + str(evaluator.evaluate(predictions_train)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a53ddbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,1.3684245806422722E-26...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,8.711591311028575E-30,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,8.190088701806704E-30,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| Fashion|[1.0,3.7862126309323696E-34...|  1.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,3.143591723372227E-40,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,6.691698128707452E-42,...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,1.6843244883576324E-43...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,9.08148115365893E-44,6...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| Fashion|[1.0,2.529493029767865E-44,...|  1.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| science|[1.0,1.8005659121365814E-50...|  3.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|               value|category|               words|           filtered1|            filtered|         rawFeatures|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, ball, tue...|(1000,[7,19,22,76...|(1000,[7,19,22,76...|  1.0|[-1576.1137468599...|[1.33931490475594...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, show, tod...|(1000,[4,7,17,19,...|(1000,[4,7,17,19,...|  1.0|[-985.37884478579...|[1.35293529545299...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, mayor, us...|(1000,[10,19,21,2...|(1000,[10,19,21,2...|  1.0|[-1868.4213233773...|[0.99999999999965...|       0.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, prosperit...|(1000,[12,15,19,3...|(1000,[12,15,19,3...|  1.0|[-1420.7633649576...|[1.37894160910707...|       2.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, westward,...|(1000,[10,47,78,8...|(1000,[10,47,78,8...|  1.0|[-1185.5997973059...|[1.0,3.7862126309...|       0.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, blue, gra...|(1000,[9,19,71,78...|(1000,[9,19,71,78...|  1.0|[-1074.9445490888...|[1.04951422840584...|       3.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, finance, ...|(1000,[38,74,78,8...|(1000,[38,74,78,8...|  1.0|[-1136.2990449351...|[0.99944885431134...|       0.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, luncheon,...|(1000,[19,21,55,7...|(1000,[19,21,55,7...|  1.0|[-1147.2057129434...|[7.52647103393057...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, speyer, h...|(1000,[30,38,71,7...|(1000,[30,38,71,7...|  1.0|[-1490.1528620572...|[9.32192836578285...|       1.0|\n",
      "|   Sections SEARC...| Fashion|[sections, search...|[sections, search...|[today, st, barna...|(1000,[19,30,38,6...|(1000,[19,30,38,6...|  1.0|[-1243.3056364292...|[6.05893301001110...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of test data using naive_bayes-----: 58.05673579090003%\n"
     ]
    }
   ],
   "source": [
    "predictions3 = model.transform(testData)\n",
    "predictions3.filter(predictions3['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "predictions3.show(10)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using naive_bayes-----: \" + str(evaluator.evaluate(predictions3)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da10c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9800785908270826,0.01379...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.9054009410715822,0.09337...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.7384371853234606,0.12491...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| science|[0.5682558978134179,0.02578...|  1.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "\n",
      "-------Accuracy of unknown data using naive_bayes-----: 39.309603440038224%\n"
     ]
    }
   ],
   "source": [
    "predictions4 = model.transform(unknown_dataset)\n",
    "predictions4.filter(predictions4['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using naive_bayes-----: \" + str(evaluator.evaluate(predictions4)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5efa4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+-----------------+-----+----------+\n",
      "|                         value|category|      probability|label|prediction|\n",
      "+------------------------------+--------+-----------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "+------------------------------+--------+-----------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of train data using Decision Tree-----: 69.41282699615432%\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                         value|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|Technology|             [1.0,0.0,0.0,0.0]|  2.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|Technology|             [1.0,0.0,0.0,0.0]|  2.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|Technology|             [1.0,0.0,0.0,0.0]|  2.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|Technology|             [1.0,0.0,0.0,0.0]|  2.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|Technology|             [1.0,0.0,0.0,0.0]|  2.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|             [1.0,0.0,0.0,0.0]|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9841269841269841,0.0,0.0...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9841269841269841,0.0,0.0...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   science|[0.9841269841269841,0.0,0.0...|  3.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|     Movie|[0.9841269841269841,0.0,0.0...|  0.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of test data using Decision Tree-----: 54.81727028249361%\n",
      "+------------------------------+----------+-----------------+-----+----------+\n",
      "|                         value|  category|      probability|label|prediction|\n",
      "+------------------------------+----------+-----------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|technology|[0.5,0.0,0.5,0.0]|  2.0|       0.0|\n",
      "+------------------------------+----------+-----------------+-----+----------+\n",
      "\n",
      "-------Accuracy of unknown data using Decision Tree-----: 27.60057333970377%\n"
     ]
    }
   ],
   "source": [
    "#training the data using Decision Tree Classifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "pipelineFit_dt = pipeline.fit(data)\n",
    "dataset = pipelineFit_dt.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "dt = DecisionTreeClassifier(impurity=\"gini\")\n",
    "dtModel = dt.fit(trainingData)\n",
    "\n",
    "predictions_dt = dtModel.transform(trainingData)\n",
    "predictions_dt.filter(predictions_dt['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of train data using Decision Tree-----: \" + str(evaluator.evaluate(predictions_dt)*100)+\"%\")\n",
    "\n",
    "\n",
    "predictions = dtModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using Decision Tree-----: \" + str(evaluator.evaluate(predictions)*100)+\"%\")\n",
    "\n",
    "\n",
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)\n",
    "predictions2 = dtModel.transform(unknown_dataset)\n",
    "predictions2.filter(predictions2['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using Decision Tree-----: \" + str(evaluator.evaluate(predictions2)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84ecab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6840458090651829,0.13882...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6683217342576837,0.11471...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6343212396708592,0.14489...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6341318613026632,0.15792...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6098285186402571,0.14259...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6018263398945013,0.16046...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5964096512402315,0.16659...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5924842959358594,0.15568...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5882397865774528,0.15451...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5877669901836134,0.16483...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of train data using Random Forest-----: 69.75588918298101%\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5839293078485153,0.15775...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5805130088713412,0.16647...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5762462717961353,0.14463...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5580950688814792,0.19304...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5450294048531371,0.16624...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5173233842512408,0.17426...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5067489945520227,0.20387...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.49407464508084986,0.1722...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.485444888236007,0.182040...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.4850028842812084,0.21302...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of test data using Random Forest-----: 51.75690926223779%\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.3514641832239275,0.25433...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.34218339206202997,0.2650...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.33968890302020155,0.2490...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.33919492069207535,0.2857...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| Fashion|[0.33862742158468734,0.2607...|  3.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.3350172102449391,0.27182...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.3346913268857182,0.24276...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.33343730878438343,0.2553...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| science|[0.3333569241396416,0.26842...|  1.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| science|[0.33214658539608044,0.2764...|  1.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of unknown data using Decision Tree-----: 35.005354465624336%\n"
     ]
    }
   ],
   "source": [
    "#training the data using Random Forest Classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "pipelineFit_rf = pipeline.fit(data)\n",
    "dataset = pipelineFit_rf.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "rf = RandomForestClassifier(numTrees=50)\n",
    "rfModel = rf.fit(trainingData)\n",
    "\n",
    "predictions_rf = rfModel.transform(trainingData)\n",
    "predictions_rf.filter(predictions_rf['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of train data using Random Forest-----: \" + str(evaluator.evaluate(predictions_rf)*100)+\"%\")\n",
    "\n",
    "\n",
    "\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using Random Forest-----: \" + str(evaluator.evaluate(predictions)*100)+\"%\")\n",
    "\n",
    "\n",
    "\n",
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)\n",
    "predictions2 = rfModel.transform(unknown_dataset)\n",
    "predictions2.filter(predictions2['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using Decision Tree-----: \" + str(evaluator.evaluate(predictions2)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "736391ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6840458090651829,0.13882...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6683217342576837,0.11471...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6343212396708592,0.14489...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6341318613026632,0.15792...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6098285186402571,0.14259...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.6018263398945013,0.16046...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5964096512402315,0.16659...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5924842959358594,0.15568...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5882397865774528,0.15451...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5877669901836134,0.16483...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of train data using Support Vector Machines-----: 69.75588918298101%\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5839293078485153,0.15775...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5805130088713412,0.16647...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5762462717961353,0.14463...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5580950688814792,0.19304...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5450294048531371,0.16624...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5173233842512408,0.17426...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.5067489945520227,0.20387...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.49407464508084986,0.1722...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.485444888236007,0.182040...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.4850028842812084,0.21302...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of test data using Support Vector Machines-----: 51.75690926223779%\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                         value|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.3514641832239275,0.25433...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.34218339206202997,0.2650...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.33968890302020155,0.2490...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.33919492069207535,0.2857...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| Fashion|[0.33862742158468734,0.2607...|  3.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.3350172102449391,0.27182...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.3346913268857182,0.24276...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...|   Movie|[0.33343730878438343,0.2553...|  0.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| science|[0.3333569241396416,0.26842...|  1.0|       0.0|\n",
      "|   Sections SEARCH Skip to ...| science|[0.33214658539608044,0.2764...|  1.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------Accuracy of unknown data using Decision Tree-----: 35.005354465624336%\n"
     ]
    }
   ],
   "source": [
    "#training the data using Support Vector Machines\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "pipelineFit_svc = pipeline.fit(data)\n",
    "dataset = pipelineFit_svc.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "svc = RandomForestClassifier(numTrees=50)\n",
    "svcModel = svc.fit(trainingData)\n",
    "\n",
    "predictions_svc = svcModel.transform(trainingData)\n",
    "predictions_svc.filter(predictions_svc['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of train data using Support Vector Machines-----: \" + str(evaluator.evaluate(predictions_svc)*100)+\"%\")\n",
    "\n",
    "\n",
    "\n",
    "predictions = svcModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of test data using Support Vector Machines-----: \" + str(evaluator.evaluate(predictions)*100)+\"%\")\n",
    "\n",
    "\n",
    "\n",
    "pipelineFit2 = pipeline.fit(unknown_data)\n",
    "unknown_dataset = pipelineFit2.transform(unknown_data)\n",
    "predictions2 = svcModel.transform(unknown_dataset)\n",
    "predictions2.filter(predictions2['prediction'] == 0) \\\n",
    "    .select(\"value\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"-------Accuracy of unknown data using Decision Tree-----: \" + str(evaluator.evaluate(predictions2)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e63a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239de4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
